{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063c6022",
   "metadata": {},
   "source": [
    "\n",
    "# Introdução ao SVM\n",
    "### Scikit-Learn (Linear, Polinomial e RBF)\n",
    "\n",
    "Este notebook é um **artefato autocontido** para estudo de *Support Vector Machines (SVM)* com `scikit-learn`.  \n",
    "Objetivos principais:\n",
    "- Compreender o hiperplano de separação e a **margem máxima** (SVM linear).\n",
    "- Visualizar **vetores de suporte** e sua influência na fronteira de decisão.\n",
    "- Explorar **separação não linear** com *kernel* **Polinomial** e **RBF**.\n",
    "- Comparar comportamentos e interpretar os gráficos gerados pelo modelo.\n",
    "\n",
    "> Dica: execute as células em ordem. Cada seção inclui orientações passo a passo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57803682",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Preparação do ambiente\n",
    "Nesta seção importamos bibliotecas e definimos funções auxiliares de visualização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f247133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports essenciais\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    \"\"\"\n",
    "    Desenha os pontos (X, y), a fronteira de decisão e as margens (-1, 0, +1).\n",
    "    - `clf` pode ser um Pipeline (com StandardScaler + SVC) ou um SVC puro.\n",
    "    - A função usa `clf.decision_function` para contornar dados e,\n",
    "      quando possível, plota os vetores de suporte no espaço original.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    # Dispersão dos dados (sem cores fixas)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=25)\n",
    "\n",
    "    # Malha para os contornos\n",
    "    xlim = (X[:,0].min() - 1, X[:,0].max() + 1)\n",
    "    ylim = (X[:,1].min() - 1, X[:,1].max() + 1)\n",
    "    xx = np.linspace(xlim[0], xlim[1], 300)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 300)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "    # A Pipeline expõe decision_function diretamente (aplica o scaler internamente)\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Contornos em nível -1, 0 (hiperplano), +1\n",
    "    ax.contour(XX, YY, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Vetores de suporte em coordenadas originais, se possível\n",
    "    svc = clf\n",
    "    scaler = None\n",
    "    if hasattr(clf, \"named_steps\"):\n",
    "        svc = clf.named_steps.get(\"svc\", None)\n",
    "        scaler = clf.named_steps.get(\"standardscaler\", None)\n",
    "\n",
    "    if svc is not None and hasattr(svc, \"support_vectors_\"):\n",
    "        sv = svc.support_vectors_\n",
    "        # Se houve padronização, traz os vetores de suporte de volta ao espaço original\n",
    "        if scaler is not None:\n",
    "            sv = scaler.inverse_transform(sv)\n",
    "        ax.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', label=\"Vetores de suporte\")\n",
    "        ax.legend()\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_and_report(clf, X_train, X_test, y_train, y_test, title):\n",
    "    \"\"\"Treina, mede acurácia e mostra matriz de confusão do classificador.\"\"\"\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Acurácia em teste — {title}: {acc:.3f}\")\n",
    "    # Remover os comentários a seguir após a aula de métricas de avaliação\n",
    "    # fig, ax = plt.subplots(figsize=(5,5))\n",
    "    # ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(ax=ax, colorbar=False)\n",
    "    # ax.set_title(f\"Matriz de Confusão — {title}\")\n",
    "    # plt.show()\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe149",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Conjuntos de dados\n",
    "Vamos criar dois conjuntos de dados 2D:\n",
    "- **Blobs**: dados aproximadamente separáveis de forma linear (útil para SVM linear).\n",
    "- **Círculos**: dados não linearmente separáveis no espaço original (motivação para kernels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c207c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset 1: Linearmente separável (blobs)\n",
    "X_lin, y_lin = datasets.make_blobs(\n",
    "    n_samples=200, centers=2, cluster_std=1.1, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 2: Não linearmente separável (círculos concêntricos)\n",
    "X_cir, y_cir = datasets.make_circles(\n",
    "    n_samples=200, factor=0.35, noise=0.06, random_state=7\n",
    ")\n",
    "\n",
    "# Visualização rápida (um gráfico por vez)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_lin[:,0], X_lin[:,1], c=y_lin, s=25)\n",
    "ax.set_title(\"Pré-visualização — Dados 'blobs' (tendem à separação linear)\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_cir[:,0], X_cir[:,1], c=y_cir, s=25)\n",
    "ax.set_title(\"Pré-visualização — Dados 'círculos' (não linear)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe492965",
   "metadata": {},
   "source": [
    "\n",
    "## 3) SVM Linear — Hiperplano e Margem\n",
    "\n",
    "- Objetivo: encontrar um **hiperplano** que separe as classes com **margem máxima**.\n",
    "- O problema primal minimiza \\( $\\frac{1}{2}\\lVert w\\rVert^2$ \\) sujeito às restrições \\( $y_i (w\\cdot x_i + b) \\ge 1 $\\).\n",
    "- **Intuição**: margens maiores tendem a melhorar a **generalização**.\n",
    "\n",
    "Vamos treinar um SVM linear e visualizar:\n",
    "1. Pontos do conjunto;\n",
    "2. **Fronteira de decisão** (nível 0);\n",
    "3. **Margens** (níveis -1 e +1);\n",
    "4. **Vetores de suporte** (destacados como círculos sem preenchimento).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ad2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treino/Teste para o conjunto linear\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_lin, y_lin, test_size=0.3, random_state=0, stratify=y_lin)\n",
    "\n",
    "# Pipeline para manter boas práticas (padronização + SVC linear)\n",
    "svm_linear = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=1.0))\n",
    "\n",
    "# Treina e reporta métricas\n",
    "svm_linear = train_and_report(svm_linear, Xtr, Xte, ytr, yte, title=\"SVM Linear\")\n",
    "\n",
    "# Visualização das margens/hiperplano\n",
    "plot_decision_boundary(svm_linear, X_lin, y_lin, \"SVM Linear — Fronteira e Margens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b21bc",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Kernel Polinomial — Fronteiras não lineares de ordem finita\n",
    "\n",
    "- Quando a separação linear **não é suficiente**, podemos **mapear implicitamente** os dados para um espaço de maior dimensão.\n",
    "- O *kernel polinomial* efetivamente considera **interações de ordem superior** entre atributos.\n",
    "- Parâmetro relevante: `degree` (grau do polinômio).\n",
    "\n",
    "Abaixo, aplicamos SVM com *kernel* polinomial aos dados em círculos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af27da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_cir, y_cir, test_size=0.3, random_state=0, stratify=y_cir)\n",
    "\n",
    "svm_poly = make_pipeline(StandardScaler(), SVC(kernel=\"poly\", degree=3, C=1.0))\n",
    "svm_poly = train_and_report(svm_poly, Xtr, Xte, ytr, yte, title=\"SVM Polinomial (grau 3)\")\n",
    "\n",
    "plot_decision_boundary(svm_poly, X_cir, y_cir, \"SVM Polinomial (grau 3) — Fronteira Não Linear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d5805",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Kernel RBF — Fronteiras flexíveis com similaridade radial\n",
    "\n",
    "- O *kernel* **RBF** (Radial Basis Function) cria fronteiras **altamente flexíveis**, úteis quando a estrutura é complexa.\n",
    "- Parâmetros relevantes: `gamma` (largura da base radial) e `C` (penalização a erros).\n",
    "- **Heurística**: valores altos de `gamma` deixam a fronteira mais sinuosa (risco de sobreajuste); valores baixos suavizam a fronteira.\n",
    "\n",
    "Aplicaremos o RBF no mesmo conjunto de círculos para comparação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ada12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_rbf = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=1.0, C=1.0))\n",
    "svm_rbf = train_and_report(svm_rbf, Xtr, Xte, ytr, yte, title=\"SVM RBF (gamma=1.0, C=1.0)\")\n",
    "\n",
    "plot_decision_boundary(svm_rbf, X_cir, y_cir, \"SVM RBF — Fronteira Não Linear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90668866",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Comparação e Reflexão Didática\n",
    "\n",
    "- **Linear**: funciona bem quando há uma separação aproximadamente reta; interpreta-se claramente a margem.\n",
    "- **Polinomial**: introduz fronteiras curvilíneas de **ordem finita**, útil quando há interações moderadas entre atributos.\n",
    "- **RBF**: fornece a maior flexibilidade; normalmente tem **bom desempenho prático**, mas exige cuidado na escolha de hiperparâmetros.\n",
    "\n",
    "> Exercício sugerido: altere `C`, `degree` e `gamma` e observe como a fronteira muda.  \n",
    "> Reflita sobre o impacto desses parâmetros na **complexidade do classificador** e no risco de **sobreajuste**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf97acb",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Exercícios práticos (autoestudo)\n",
    "\n",
    "1. **Ruído**: aumente `noise` no `make_circles` e verifique como os vetores de suporte e a acurácia mudam.\n",
    "2. **Degrau polinomial**: teste `degree=2`, `degree=4` e compare as fronteiras.\n",
    "3. **RBF fino x grosso**: varie `gamma` entre `0.1`, `0.5`, `1`, `5` e `10` e discuta o efeito.\n",
    "4. **Métrica adicional**: inclua *precision*, *recall* e *F1-score* e analise os resultados. Volte aqui após a aula de métricas de avaliação.\n",
    "5. **Projeção 2D**: gere um dataset 3D (três atributos) e use PCA para visualizar a separação em 2D. Sugestão avançada. Exige pesquisa e estudo do PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f4136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa6db5b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
