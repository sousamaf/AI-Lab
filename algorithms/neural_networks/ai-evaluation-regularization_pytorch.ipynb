{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0ea7b0",
   "metadata": {},
   "source": [
    "\n",
    "# Regularização em PyTorch: Baseline vs L2 vs L1\n",
    "\n",
    "Este notebook treina uma MLP simples em um dataset sintético de classificação binária\n",
    "e compara três cenários:\n",
    "1. **Baseline** (sem regularização)\n",
    "2. **L2** (*weight decay* no otimizador)\n",
    "3. **L1** (penalização manual na loss)\n",
    "\n",
    "São geradas curvas de **loss** e **acurácia** de treino/validação para cada cenário.\n",
    "> Observações:\n",
    "> - Somente **matplotlib** é utilizada (sem seaborn).\n",
    "> - Cada gráfico é plotado em **uma figura** (sem subplots).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb64bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixar semente para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9d6e4",
   "metadata": {},
   "source": [
    "## 1) Dataset sintético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset binário moderadamente ruidoso e não-linear\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=20, n_informative=6, n_redundant=4, n_repeated=0,\n",
    "    n_clusters_per_class=2, weights=[0.6, 0.4], class_sep=1.2, flip_y=0.02,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val   = torch.tensor(X_val,   dtype=torch.float32).to(device)\n",
    "y_val   = torch.tensor(y_val,   dtype=torch.float32).to(device)\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395bec3",
   "metadata": {},
   "source": [
    "## 2) MLP simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd46556",
   "metadata": {},
   "source": [
    "## 3) Funções de treino/avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size=128, shuffle=True):\n",
    "    n = X.shape[0]\n",
    "    idx = torch.arange(n)\n",
    "    if shuffle:\n",
    "        idx = idx[torch.randperm(n)]\n",
    "    for i in range(0, n, batch_size):\n",
    "        j = idx[i:i+batch_size]\n",
    "        yield X[j], y[j]\n",
    "\n",
    "def accuracy_from_logits(logits, y_true):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "    return (preds == y_true).float().mean().item()\n",
    "\n",
    "def train_variant(variant=\"baseline\", l2_lambda=0.0, l1_lambda=0.0, epochs=50, lr=1e-3):\n",
    "    model = MLP(n_features).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for xb, yb in batch_iter(X_train, y_train, batch_size=128, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            # L1 manual (se aplicável)\n",
    "            if l1_lambda > 0.0:\n",
    "                l1_term = torch.tensor(0.0, device=device)\n",
    "                for p in model.parameters():\n",
    "                    l1_term = l1_term + p.abs().sum()\n",
    "                loss = loss + l1_lambda * l1_term\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_acc  += accuracy_from_logits(logits, yb)\n",
    "            batches += 1\n",
    "\n",
    "        # Métricas de validação\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val)\n",
    "            val_loss = criterion(val_logits, y_val).item()\n",
    "            if l1_lambda > 0.0:\n",
    "                l1_term_val = torch.tensor(0.0, device=device)\n",
    "                for p in model.parameters():\n",
    "                    l1_term_val = l1_term_val + p.abs().sum()\n",
    "                val_loss = val_loss + l1_lambda * l1_term_val.item()\n",
    "\n",
    "            val_acc = accuracy_from_logits(val_logits, y_val)\n",
    "\n",
    "        hist[\"train_loss\"].append(running_loss / max(1, batches))\n",
    "        hist[\"train_acc\"].append(running_acc / max(1, batches))\n",
    "        hist[\"val_loss\"].append(val_loss)\n",
    "        hist[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    return model, hist\n",
    "\n",
    "def plot_curves(hist, title_prefix=\"Baseline\", dpi=180, save_prefix=None):\n",
    "    epochs = np.arange(1, len(hist[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"Treino (loss)\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"],   label=\"Validação (loss)\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} — Curva de Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.25)\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_loss.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Acurácia\n",
    "    plt.figure(figsize=(7.2, 4.2))\n",
    "    plt.plot(epochs, hist[\"train_acc\"], label=\"Treino (acurácia)\")\n",
    "    plt.plot(epochs, hist[\"val_acc\"],   label=\"Validação (acurácia)\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Acurácia\")\n",
    "    plt.title(f\"{title_prefix} — Curva de Acurácia\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.25)\n",
    "    if save_prefix:\n",
    "        plt.savefig(f\"{save_prefix}_accuracy.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82491d6",
   "metadata": {},
   "source": [
    "## 4) Treinos: Baseline, L2 e L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612780d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "LR = 1e-3\n",
    "DPI = 180\n",
    "\n",
    "# Baseline (sem regularização)\n",
    "model_base, hist_base = train_variant(\"baseline\", l2_lambda=0.0, l1_lambda=0.0, epochs=EPOCHS, lr=LR)\n",
    "plot_curves(hist_base, title_prefix=\"Baseline\", dpi=DPI, save_prefix=\"regularization_baseline\")\n",
    "\n",
    "# L2 (weight decay)\n",
    "model_l2, hist_l2 = train_variant(\"l2\", l2_lambda=1e-3, l1_lambda=0.0, epochs=EPOCHS, lr=LR)\n",
    "plot_curves(hist_l2, title_prefix=\"L2 (weight decay)\", dpi=DPI, save_prefix=\"regularization_l2\")\n",
    "\n",
    "# L1 (penalização manual)\n",
    "model_l1, hist_l1 = train_variant(\"l1\", l2_lambda=0.0, l1_lambda=1e-6, epochs=EPOCHS, lr=LR)\n",
    "plot_curves(hist_l1, title_prefix=\"L1\", dpi=DPI, save_prefix=\"regularization_l1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71fae4",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Observações\n",
    "\n",
    "- **L2** tende a estabilizar os pesos e reduzir oscilações, geralmente ajudando a manter a\n",
    "  curva de validação mais próxima da de treino.\n",
    "- **L1** induz esparsidade, podendo simplificar a hipótese aprendida; a escolha do `l1_lambda`\n",
    "  é sensível — valores muito altos podem prejudicar o aprendizado.\n",
    "- Ajuste `DPI` nas funções `savefig` dentro de `plot_curves` para controlar a qualidade.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
