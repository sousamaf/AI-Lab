{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723ef302",
   "metadata": {},
   "source": [
    "# Fine-tuning Didático de um Modelo GPT (OSS) com LoRA (PEFT)\n",
    "\n",
    "Este notebook demonstra, de forma **didática e prática**, como realizar **fine-tuning** de um modelo **GPT open-source** usando a biblioteca **Hugging Face Transformers** em conjunto com **PEFT (LoRA)** para tornar o processo **viável em hardware modesto**.\n",
    "\n",
    "> **Importante:** Modelos com ~20B de parâmetros **exigem infraestrutura robusta** (GPUs de alta memória, quantização, FSDP/DeepSpeed, etc.). Neste notebook, usamos um **modelo menor** para ilustrar os conceitos e a pipeline completa.\n",
    "\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "- Compreender o fluxo de fine-tuning para **modelos de linguagem causal (GPT-like)**.\n",
    "- Preparar dados curtos (instrução → resposta) e **tokenizar** corretamente.\n",
    "- Aplicar **LoRA (PEFT)** para reduzir custo de ajuste fino.\n",
    "- Treinar, avaliar com **geração** e salvar o adaptador.\n",
    "- Carregar o adaptador para **inferência** pós-treino.\n",
    "\n",
    "Implementação baseada em:\n",
    "[gpt-oss-(20B)-Fine-tuning](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb#scrollTo=waDcYbdVUesj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec59ef",
   "metadata": {},
   "source": [
    "## Requisitos e Instalação\n",
    "\n",
    "Execute a célula abaixo **se** ainda não possuir as dependências instaladas. \n",
    "O conjunto mínimo inclui:\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `accelerate`\n",
    "- `peft`\n",
    "- `evaluate` (opcional, para métricas)\n",
    "- `bitsandbytes` (opcional; útil para quantização em GPU NVIDIA)\n",
    "\n",
    "> Observação: se estiver no Google Colab, selecione **Runtime → Change runtime type → GPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação (descomente se necessário)\n",
    "# !pip install -U transformers datasets accelerate peft evaluate\n",
    "# !pip install bitsandbytes  # (opcional; funciona com GPUs NVIDIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f440df",
   "metadata": {},
   "source": [
    "## Verificação de Dispositivo (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Dispositivo:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"Nome da GPU:\", torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc09383",
   "metadata": {},
   "source": [
    "## Escolha do Modelo Base\n",
    "\n",
    "Para fins didáticos, escolha um **modelo pequeno** compatível com `AutoModelForCausalLM`. \n",
    "Alguns exemplos (substitua `model_name` abaixo):\n",
    "- `gpt2` (inglês, pequeno)\n",
    "- `uer/gpt2-chinese-cluecorpussmall` (chinês, exemplo multicultural)\n",
    "- `EleutherAI/pythia-70m-deduped` (família Pythia)\n",
    "- `openai-community/gpt2` (espelho comunitário do GPT-2)\n",
    "\n",
    "> Para modelos **maiores** (p.ex., família Falcon/OPT/Mistral/Llama), é recomendada quantização, FSDP/DeepSpeed e múltiplas GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Modelo base pequeno para fins ilustrativos\n",
    "model_name = \"gpt2\"  # altere conforme desejado\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 não possui token pad por padrão; definimos um\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "print(\"Modelo carregado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b8513",
   "metadata": {},
   "source": [
    "## Preparação de Dados (Exemplo Simples)\n",
    "\n",
    "Abaixo, definimos um **mini dataset** de pares *(instrução → resposta)* em português para ilustrar o processo. \n",
    "Em um cenário real, você pode carregar:\n",
    "- Arquivos JSON/CSV com colunas `instruction` e `output`\n",
    "- Conjuntos do `datasets` (Hugging Face Hub)\n",
    "- Seu próprio dataset institucional\n",
    "\n",
    "Vamos formatar os exemplos como *prompt* concatenado com *resposta*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefab750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Mini dataset ilustrativo (INSTRUÇÃO -> RESPOSTA)\n",
    "samples = [\n",
    "    {\"instruction\": \"Explique o que é Aprendizado de Máquina em 1 frase.\", \n",
    "     \"output\": \"É a área que estuda algoritmos capazes de aprender padrões a partir de dados.\"},\n",
    "    {\"instruction\": \"Dê um exemplo de transformação linear.\", \n",
    "     \"output\": \"Uma matriz multiplicando um vetor no espaço vetorial é um exemplo de transformação linear.\"},\n",
    "    {\"instruction\": \"O que é overfitting?\", \n",
    "     \"output\": \"É quando o modelo memoriza o conjunto de treino e generaliza mal para novos dados.\"},\n",
    "    {\"instruction\": \"Defina rede neural.\", \n",
    "     \"output\": \"É um modelo composto por camadas de neurônios artificiais interconectados que aprendem representações.\"},\n",
    "    {\"instruction\": \"Cite um caso de uso de Processamento de Linguagem Natural.\", \n",
    "     \"output\": \"Análise de sentimento de avaliações de usuários é um caso comum de PLN.\"},\n",
    "]\n",
    "\n",
    "def build_prompt(example):\n",
    "    return f\"Instrução: {example['instruction']}\\nResposta:\"\n",
    "\n",
    "def build_target(example):\n",
    "    return example[\"output\"]\n",
    "\n",
    "dataset = Dataset.from_list([\n",
    "    {\"text\": build_prompt(s) + \" \" + build_target(s)}\n",
    "    for s in samples\n",
    "])\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee89b1",
   "metadata": {},
   "source": [
    "## Tokenização e Collator\n",
    "\n",
    "A tokenização converte o texto em IDs de tokens do vocabulário. Para modelos causais (GPT-like), \n",
    "o rótulo para treinamento é geralmente o **próprio texto deslocado** (causal LM). \n",
    "Usaremos `DataCollatorForLanguageModeling` com `mlm=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86abab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=256, \n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Split train/eval simples\n",
    "split = tokenized.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "len(train_ds), len(eval_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e283b0",
   "metadata": {},
   "source": [
    "## Configuração de PEFT (LoRA)\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** permite treinar **apenas pequenos adaptadores** em algumas camadas do modelo, \n",
    "reduzindo **drasticamente** o número de parâmetros atualizados e o custo computacional.\n",
    "\n",
    "Parâmetros comuns:\n",
    "- `r`: rank da decomposição (ex.: 8, 16, 32)\n",
    "- `alpha`: escala da atualização (ex.: 16, 32)\n",
    "- `target_modules`: quais projeções receberão LoRA (ex.: `c_attn`, `q_proj`, `v_proj` dependendo do modelo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuração LoRA básica (ajuste conforme o modelo)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],  # para GPT-2; para outros modelos pode ser [\"q_proj\",\"v_proj\"]\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89905b",
   "metadata": {},
   "source": [
    "## Treinamento (Hugging Face Trainer)\n",
    "\n",
    "Usaremos o `Trainer` para simplificar o loop de treinamento. Ajuste os hiperparâmetros conforme o ambiente.\n",
    "\n",
    "> Dica: Em GPU com pouca memória, reduza `per_device_train_batch_size`, `max_length` e `r` do LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from inspect import signature\n",
    "import math, torch, transformers\n",
    "\n",
    "print(\"Transformers versão:\", transformers.__version__)\n",
    "\n",
    "output_dir = \"gpt2-lora-pt\"\n",
    "\n",
    "# kwargs padrão (para versões novas)\n",
    "ta_kwargs = dict(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # <– pode não existir em versões antigas/carregadas\n",
    "    eval_steps=10,\n",
    "    save_steps=20,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),   # só CUDA\n",
    "    bf16=False,                       # mantenha False no MPS\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Adaptação automática se a assinatura não tiver 'evaluation_strategy'\n",
    "params = signature(TrainingArguments).parameters\n",
    "if \"evaluation_strategy\" not in params:\n",
    "    # Remover chaves não suportadas\n",
    "    ta_kwargs.pop(\"evaluation_strategy\", None)\n",
    "    ta_kwargs.pop(\"eval_steps\", None)\n",
    "    # Alternativa para versões antigas: apenas permitir avaliação manual depois\n",
    "    ta_kwargs[\"do_eval\"] = True\n",
    "    print(\"Atenção: 'evaluation_strategy' não suportado nesta classe carregada. Usando fallback com do_eval=True.\")\n",
    "\n",
    "training_args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "if \"eval_loss\" in eval_metrics:\n",
    "    try:\n",
    "        print(\"Perplexity:\", math.exp(eval_metrics[\"eval_loss\"]))\n",
    "    except OverflowError:\n",
    "        print(\"Perplexity overflow (loss muito grande).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb637318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4fbcb",
   "metadata": {},
   "source": [
    "## Avaliação Informal por Geração\n",
    "\n",
    "Após o fine-tuning, fazemos uma **geração** a partir de uma instrução não vista, \n",
    "para verificar se o modelo aprendeu o formato *Instrução → Resposta*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780368e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0\n",
    "elif hasattr(torch, \"mps\") and torch.mps.is_available():\n",
    "    device_id = \"mps\" \n",
    "else:\n",
    "    device_id = -1  # CPU\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device_id\n",
    ")\n",
    "\n",
    "prompt = \"Instrução: Explique o que é regularização em aprendizado de máquina.\\nResposta:\"\n",
    "gen = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(gen[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbb380",
   "metadata": {},
   "source": [
    "## Salvamento e Carregamento do Adaptador LoRA\n",
    "\n",
    "Salvamos o **adaptador LoRA** (parâmetros aprendidos). Para reutilizar:\n",
    "1. Carregar o mesmo modelo base.\n",
    "2. Aplicar `PEFT` com `from_pretrained` no diretório salvo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8693ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "adapter_dir = Path(\"gpt2-lora-pt\") / \"lora_adapter\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "print(\"Adaptador LoRA salvo em:\", adapter_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2ca51",
   "metadata": {},
   "source": [
    "## Como usar o adaptador em outro ambiente\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Carregar o adaptador salvo\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"gpt2-lora-pt/lora_adapter\")\n",
    "\n",
    "# Inferência\n",
    "peft_model.eval()\n",
    "```\n",
    "\n",
    "## Depois de carregar o modelo, pode testar com o seguinte trecho\n",
    "```python\n",
    "prompt = \"Instrução: Explique o que é regularização em aprendizado de máquina.\\nResposta:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
    "outputs = peft_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "> Em produção, considere quantização (8-bit/4-bit com `bitsandbytes`), FSDP/DeepSpeed e *checkpointing* de ativação para modelos maiores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494b68f",
   "metadata": {},
   "source": [
    "## Boas Práticas e Adaptação para Modelos Maiores (~20B)\n",
    "\n",
    "- **Quantização**: 8-bit/4-bit (`bitsandbytes`) para reduzir memória.\n",
    "- **Treino distribuído**: FSDP / ZeRO / DeepSpeed para particionar pesos/ótimos/gradientes.\n",
    "- **Checkpointing de ativação**: reduz memória com custo de recálculo.\n",
    "- **Gradiente acumulado** + **batch pequeno**: compatibiliza com GPUs menores.\n",
    "- **Mixed Precision** (`fp16`/`bf16`): acelera e reduz memória (requer suporte de hardware).\n",
    "- **Data quality**: curadoria dos exemplos (instruções claras, respostas curtas e diretas).\n",
    "- **Avaliação**: defina *prompts* de validação replicáveis e métricas (p.ex., perplexidade).\n",
    "\n",
    "> **Infra**: para 20B+, recomenda-se múltiplas GPUs (A100, H100, etc.), armazenamento rápido e orquestração (Slurm, Ray, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389508a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
