{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210443fd",
   "metadata": {},
   "source": [
    "\n",
    "# Mini Transformer (Frases Curtas) – PyTorch\n",
    "\n",
    "Este notebook demonstra, de forma **didática e leve**, como treinar um **mini Transformer** para **completar frases curtas** (nível de palavras) usando **PyTorch**.  \n",
    "A proposta é construir um **modelo decoder-only simplificado** (usando `TransformerEncoder` com **máscara causal**) que aprende padrões básicos de sequência e consegue **prever a próxima palavra** a partir de um *prompt* curto.\n",
    "\n",
    "**Você verá**:\n",
    "- Como montar um **corpus mínimo** de frases curtas em PT-BR.\n",
    "- Como construir um **vocabulário** e preparar tensores para treino.\n",
    "- Como implementar um **MiniTransformerLM** (embedding + positional encoding + `TransformerEncoder` + cabeça linear).\n",
    "- Como **treinar** rapidamente e **gerar texto** (autocompletar frases).\n",
    "\n",
    "> **Observação didática**: este notebook é ideal para turmas de anos iniciais **entenderem o fluxo** (dados → modelo → treino → geração) e **desmistificar** o uso do Transformer em pequena escala.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9014230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch --quiet  # Se estiver no Colab e precisar instalar\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb6a65",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Corpus mínimo (frases curtas)\n",
    "\n",
    "Vamos criar um conjunto **pequeno** de frases curtas em português.  \n",
    "A ideia é permitir **treinos rápidos** (1–5 minutos) e ainda assim ver o modelo **aprender padrões** simples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"o gato dorme\",\n",
    "    \"o cachorro corre\",\n",
    "    \"a menina sorri\",\n",
    "    \"a menina corre\",\n",
    "    \"o menino sorri\",\n",
    "    \"o menino corre\",\n",
    "    \"o gato mia\",\n",
    "    \"o cachorro late\",\n",
    "    \"a menina pula\",\n",
    "    \"o menino pula\",\n",
    "    \"o gato pula\",\n",
    "    \"o cachorro dorme\",\n",
    "    \"o aluno estuda\",\n",
    "    \"a aluna estuda\",\n",
    "    \"o professor explica\",\n",
    "    \"a professora explica\",\n",
    "    \"o aluno aprende\",\n",
    "    \"a aluna aprende\",\n",
    "    \"o livro cai\",\n",
    "    \"a bola rola\",\n",
    "    \"o carro anda\",\n",
    "    \"o carro para\",\n",
    "    \"a luz acende\",\n",
    "    \"a luz apaga\",\n",
    "    \"a criança brinca\",\n",
    "    \"o bebê dorme\",\n",
    "]\n",
    "\n",
    "# Embaralhar (opcional) para variar a ordem entre execuções\n",
    "random.seed(42)\n",
    "random.shuffle(corpus)\n",
    "\n",
    "len(corpus), corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9f3aa",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Tokenização e Vocabulário (nível de palavra)\n",
    "\n",
    "Usaremos uma **tokenização simples por espaço** e adicionaremos tokens especiais:\n",
    "- `<pad>`: padding (alinhamento dos comprimentos)\n",
    "- `<bos>`: início de sequência\n",
    "- `<eos>`: final de sequência\n",
    "\n",
    "O modelo receberá como entrada `bos + tokens_da_frase` e aprenderá a **prever o próximo token** em cada passo (linguagem autoregressiva).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f69c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # lowercase\n",
    "    s = s.lower().strip()\n",
    "    # opcional: remover acentos (comente se não quiser)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # separar pontuação simples (mantém vírgulas/pontos como tokens)\n",
    "    s = re.sub(r\"([,.;:!?])\", r\" \\1 \", s)\n",
    "    # colapsar espaços\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    return normalize_text(sentence).split()\n",
    "\n",
    "# Reconstruir corpus normalizado (mantém o texto original apenas para referência)\n",
    "corpus_norm = [normalize_text(s) for s in corpus]\n",
    "\n",
    "# Construir vocabulário com <unk>\n",
    "vocab_set = {PAD, BOS, EOS, UNK}\n",
    "for s in corpus_norm:\n",
    "    vocab_set.update(tokenize(s))\n",
    "\n",
    "itos = sorted(list(vocab_set))               # index -> string\n",
    "stoi: Dict[str, int] = {tok: i for i, tok in enumerate(itos)}  # string -> index\n",
    "vocab_size = len(itos)\n",
    "\n",
    "itos[:10], vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daf42e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dataset e DataLoader\n",
    "\n",
    "Vamos preparar tensores de entrada e saída.  \n",
    "Dado um exemplo `\"o gato dorme\"`, o **input** será `\"<bos> o gato dorme\"` e o **target** será `\"o gato dorme <eos>\"`.  \n",
    "Assim, o modelo aprende a prever o **próximo token** a cada posição (treino autoregressivo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens: List[str]) -> List[int]:\n",
    "    return [stoi.get(t, stoi[UNK]) for t in tokens]\n",
    "\n",
    "def detok(ids: List[int]) -> str:\n",
    "    toks = [itos[i] for i in ids if i != stoi[PAD]]\n",
    "    # remove <bos> e corta em <eos>\n",
    "    try:\n",
    "        if BOS in toks:\n",
    "            toks = toks[toks.index(BOS)+1:]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        if EOS in toks:\n",
    "            toks = toks[:toks.index(EOS)]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return \" \".join(toks)\n",
    "\n",
    "class TinyLMDataset(Dataset):\n",
    "    def __init__(self, sentences: List[str], max_len: int = None):\n",
    "        self.examples = []\n",
    "        tokenized = [tokenize(s) for s in sentences]\n",
    "        if max_len is None:\n",
    "            max_len = max(len(toks) for toks in tokenized) + 1  # +1 para BOS/EOS\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for toks in tokenized:\n",
    "            inp = [BOS] + toks\n",
    "            tgt = toks + [EOS]\n",
    "            self.examples.append((inp, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt = self.examples[idx]\n",
    "        return inp, tgt\n",
    "\n",
    "def pad_to_len(ids: List[int], max_len: int, pad_id: int) -> List[int]:\n",
    "    return ids + [pad_id] * (max_len - len(ids))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pad_id = stoi[PAD]\n",
    "    xs, ys = [], []\n",
    "    max_len = max(len(x) for x, _ in batch)\n",
    "    for inp, tgt in batch:\n",
    "        x_ids = encode(inp)\n",
    "        y_ids = encode(tgt)\n",
    "        x_ids = pad_to_len(x_ids, max_len, pad_id)\n",
    "        y_ids = pad_to_len(y_ids, max_len, pad_id)\n",
    "        xs.append(x_ids)\n",
    "        ys.append(y_ids)\n",
    "    return torch.tensor(xs, dtype=torch.long), torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "# Use o corpus normalizado aqui:\n",
    "dataset = TinyLMDataset(corpus_norm)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "batch_x, batch_y = next(iter(loader))\n",
    "batch_x.shape, batch_y.shape, batch_x[0], batch_y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b72f4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Modelo: MiniTransformerLM\n",
    "\n",
    "Arquitetura **simples e didática**:\n",
    "- **Embedding** de tokens\n",
    "- **Positional Encoding senoidal**\n",
    "- **TransformerEncoder** (com **máscara causal** para não olhar o futuro)\n",
    "- **Camada linear** para prever distribuição de probabilidade no vocabulário\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead384c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "def generate_causal_mask(sz: int) -> torch.Tensor:\n",
    "    # Máscara triangular superior para impedir atenção ao futuro\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "class MiniTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=256)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        emb = self.tok_emb(x)                 # (batch, seq_len, d_model)\n",
    "        h = self.pos_enc(emb)                 # + posição\n",
    "        seq_len = x.size(1)\n",
    "        mask = generate_causal_mask(seq_len).to(x.device)  # (seq_len, seq_len)\n",
    "        h = self.encoder(h, mask=mask)        # (batch, seq_len, d_model)\n",
    "        logits = self.lm_head(h)              # (batch, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c38f3f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Treinamento rápido\n",
    "\n",
    "Usaremos **CrossEntropyLoss** ignorando `<pad>` e um otimizador **Adam**.  \n",
    "Como o corpus é pequeno, poucas épocas já mostram aprendizado de padrões.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c68b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, epochs=10, lr=3e-3):\n",
    "    model = model.to(device)\n",
    "    pad_id = stoi[PAD]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)  # (B, T, V)\n",
    "            B, T, V = logits.shape\n",
    "            loss = criterion(logits.view(B*T, V), y.view(B*T))\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "        avg_loss = total_loss / steps\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch:02d} | loss: {avg_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "model = MiniTransformerLM(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, dim_feedforward=256)\n",
    "losses = train_model(model, loader, epochs=200, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e772346",
   "metadata": {},
   "source": [
    "\n",
    "### Gráfico de perda (loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c49447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Treinamento - Loss por época\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5619d06",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Geração: completar frases\n",
    "\n",
    "Função de **decodificação autoregressiva** (greedy) a partir de um *prompt* inicial.  \n",
    "Você pode testar com entradas como `\"o cachorro\"`, `\"a menina\"`, `\"o aluno\"`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detok(ids: List[int]) -> str:\n",
    "    toks = [itos[i] for i in ids if i != stoi[PAD]]\n",
    "    # Remove <bos> se aparecer e corta em <eos>\n",
    "    if BOS in toks:\n",
    "        toks = toks[toks.index(BOS)+1:] if BOS in toks else toks\n",
    "    if EOS in toks:\n",
    "        toks = toks[:toks.index(EOS)]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt: str, max_new_tokens=5):\n",
    "    model.eval()\n",
    "    toks = tokenize(prompt)\n",
    "    if not toks:\n",
    "        toks = [UNK]\n",
    "    in_tokens = [BOS] + toks\n",
    "    x = torch.tensor([encode(in_tokens)], dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(x)  # (1, T, V)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_id = torch.argmax(next_token_logits, dim=-1)\n",
    "        x = torch.cat([x, next_id.unsqueeze(0)], dim=1)\n",
    "        if next_id.item() == stoi[EOS]:\n",
    "            break\n",
    "    return detok(x[0].tolist())\n",
    "\n",
    "# Exemplos rápidos\n",
    "tests = [\"o cachorro\", \"a menina\", \"o gato\", \"o aluno\", \"a luz\"]\n",
    "for t in tests:\n",
    "    print(f\"Entrada: {t!r} -> Saída: {generate(model, t, max_new_tokens=5)!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca9b50",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Extensões sugeridas\n",
    "\n",
    "- **Aumentar o corpus** com mais frases e variações verbais.\n",
    "- **Ajustar o tamanho do modelo** (`d_model`, `nhead`, `num_layers`) para observar limites de capacidade.\n",
    "- **Trocar greedy por amostragem** (*top-k*/*top-p*) para sentenças mais diversas.\n",
    "- **Adicionar regularização** (dropout maior) para evitar overfitting no corpus minúsculo.\n",
    "- **Hiperparâmetros**: teste `lr`, `epochs`, `batch_size` e observe o impacto no gráfico de *loss*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
