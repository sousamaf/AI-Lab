{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9229c5ce",
   "metadata": {},
   "source": [
    "# Few-shot Learning na Prática (In-Context Learning)\n",
    "\n",
    "Este notebook demonstra, de forma simples, como **um modelo pré-treinado** pode aprender um **padrão de tarefa** apenas observando **alguns exemplos no próprio prompt**, sem qualquer re-treinamento.\n",
    "\n",
    "Chamamos isso de **Few-shot Learning em contexto (in-context learning)**.\n",
    "\n",
    "---\n",
    "\n",
    "## O que você vai ver neste notebook\n",
    "\n",
    "1. Como montar um **prompt com poucos exemplos** (few-shots).\n",
    "2. Como o modelo **induz a regra** a partir desses exemplos.\n",
    "3. Dois tipos de tarefa sem treinar nada:\n",
    "   - Classificação de sentimento em texto.\n",
    "   - Extração de informações (nome e idade) de frases em português.\n",
    "\n",
    "> Observação: este notebook usa modelos da biblioteca **Hugging Face Transformers**.\n",
    "> Na primeira execução, o modelo será baixado (é necessário acesso à internet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447e241",
   "metadata": {},
   "source": [
    "## 1. Preparando o ambiente\n",
    "\n",
    "Nesta célula, vamos:\n",
    "\n",
    "- Verificar a versão do Python e das bibliotecas principais.\n",
    "- Detectar se há GPU (**cuda**), **mps** (Apple Silicon) ou apenas CPU.\n",
    "\n",
    "Isso não altera o conceito de *few-shot learning*, mas ajuda a entender **onde** o modelo está sendo executado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print('Versão do Python:', sys.version.split()[0])\n",
    "print('Versão do PyTorch:', torch.__version__)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if hasattr(torch, 'mps') and torch.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print('Dispositivo em uso:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3ef1a",
   "metadata": {},
   "source": [
    "## 2. Carregando um modelo de linguagem (causal LM)\n",
    "\n",
    "Usaremos um modelo pequeno da família **GPT-2** apenas para demonstração.\n",
    "\n",
    "- Ele não é um modelo de 'chat', mas consegue **continuar textos**.\n",
    "- Vamos explorar isso para mostrar o comportamento **few-shot**.\n",
    "\n",
    "> Se você estiver em ambiente sem internet, a primeira execução pode falhar por não conseguir baixar o modelo.\n",
    "> Ainda assim, leia o código e discuta o que ele faz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = 'gpt2'\n",
    "# model_name = \"gpt2-medium\"      # ou \"gpt2-large\", \"gpt2-xl\"\n",
    "\n",
    "print('Carregando tokenizer e modelo:', model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Seleciona o device para o pipeline\n",
    "if torch.cuda.is_available():\n",
    "    dev = 0\n",
    "elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "    # algumas versões podem não suportar 'mps' diretamente no pipeline;\n",
    "    # se der erro, basta trocar para dev = -1 (CPU)\n",
    "    dev = -1\n",
    "else:\n",
    "    dev = -1\n",
    "\n",
    "text_gen = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=dev\n",
    ")\n",
    "\n",
    "print('Pipeline pronto.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d86683",
   "metadata": {},
   "source": [
    "## 3. Exemplo 1 – Classificação de sentimento via Few-shot Learning\n",
    "\n",
    "### Ideia\n",
    "\n",
    "Vamos mostrar para o modelo alguns exemplos de frases com sentimentos **POSITIVO**, **NEGATIVO** e **NEUTRO**.\n",
    "\n",
    "Depois, pedimos para ele **continuar o padrão** e classificar uma nova frase.\n",
    "\n",
    "Não estamos treinando pesos.\n",
    "Estamos apenas **mostrando o padrão no texto** e deixando o modelo *imitar*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Você é um classificador de sentimento.\n",
    "Como saída, use apenas as palavras: POSITIVO, NEGATIVO ou NEUTRO.\n",
    "\n",
    "Exemplos:\n",
    "Frase: \"O filme foi maravilhoso, adorei cada cena.\" -> POSITIVO\n",
    "Frase: \"Estou muito decepcionado com o serviço.\" -> NEGATIVO\n",
    "Frase: \"A entrega ocorreu como esperado.\" -> NEUTRO\n",
    "\n",
    "Agora classifique a frase a seguir como POSITIVO, NEGATIVO ou NEUTRO:\n",
    "Frase: \"O produto superou minhas expectativas!\" ->\n",
    "'''\n",
    "\n",
    "saida = text_gen(\n",
    "    prompt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.1,  # menor temperatura = comportamento mais previsível\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(saida[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bb35c",
   "metadata": {},
   "source": [
    "### Discussão\n",
    "\n",
    "- O modelo **não foi treinado** especificamente para classificação de sentimento.\n",
    "- Mesmo assim, ao ver **poucos exemplos** (few-shots), ele tenta **estender o padrão**.\n",
    "- O resultado pode não ser perfeito, mas ilustra bem a ideia de:\n",
    "\n",
    "> **Aprender pela configuração do prompt**, e não pela atualização de pesos.\n",
    "\n",
    "Experimente alterar:\n",
    "\n",
    "- As frases de exemplo.\n",
    "- A frase a ser classificada.\n",
    "- A instrução ('Use apenas as palavras...').\n",
    "\n",
    "Veja como isso impacta o comportamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9cce0",
   "metadata": {},
   "source": [
    "## 4. Exemplo 2 – Extração de informação (nome e idade)\n",
    "\n",
    "Agora, vamos usar few-shot learning para **extrair informações estruturadas** de frases em português.\n",
    "\n",
    "### Tarefa\n",
    "\n",
    "Dado um texto com alguém e sua idade, queremos extrair:\n",
    "\n",
    "- `Nome:`\n",
    "- `Idade:`\n",
    "\n",
    "Novamente, sem treinar nada. Apenas **mostrando exemplos no prompt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ie = '''\n",
    "Extraia o nome e a idade das frases a seguir.\n",
    "\n",
    "Exemplos:\n",
    "\"Maria tem 32 anos.\" -> Nome: Maria | Idade: 32\n",
    "\"João completou 45 anos ontem.\" -> Nome: João | Idade: 45\n",
    "\n",
    "Agora extraia os dados da frase a seguir e apresente no mesmo formato dos exemplos:\n",
    "\"Carla fará 29 anos no próximo mês.\" ->\n",
    "'''\n",
    "\n",
    "saida_ie = text_gen(\n",
    "    prompt_ie,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(saida_ie[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca78abe",
   "metadata": {},
   "source": [
    "### Discussão\n",
    "\n",
    "Observe que:\n",
    "\n",
    "- O modelo tenta **seguir o formato** dos exemplos.\n",
    "- Ele infere qualitativamente o que é **Nome** e o que é **Idade**.\n",
    "- Ele não possui uma 'regra de idade' programada, mas **induz o padrão** a partir do texto.\n",
    "\n",
    "Experimente:\n",
    "\n",
    "- Alterar os exemplos (por exemplo, adicionar mais um caso).\n",
    "- Mudar a forma do output (por exemplo, `JSON: {\"nome\": ..., \"idade\": ...}`).\n",
    "- Traduzir tudo para inglês e comparar o comportamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552daf1",
   "metadata": {},
   "source": [
    "## 5. Few-shot Learning x Fine-tuning (reflexão final)\n",
    "\n",
    "Com este notebook, você viu que:\n",
    "\n",
    "- Em **few-shot in-context learning**, o modelo não é re-treinado.\n",
    "  - Não há *backpropagation* nem atualização de pesos.\n",
    "  - O comportamento muda apenas pela **forma como construímos o prompt**.\n",
    "\n",
    "- Em **fine-tuning**, há um processo explícito de treino:\n",
    "  - Atualizamos pesos do modelo com gradiente.\n",
    "  - O modelo 'incorpora' a nova tarefa de modo permanente.\n",
    "\n",
    "### Perguntas para reflexão\n",
    "\n",
    "1. Em que cenários few-shot learning é suficiente?\n",
    "2. Quando seria necessário partir para fine-tuning?\n",
    "3. Que vantagens e limitações você observou ao apenas mudar o prompt?\n",
    "\n",
    "Use estas perguntas como base para sua participação em fórum ou relatório da disciplina."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
