{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa509d5",
   "metadata": {},
   "source": [
    "\n",
    "# MLP - Visão Guiada em **PyTorch**\n",
    "\n",
    "Esta é a **versão em PyTorch** do notebook *mlp_visao_guiada*. Mantemos a lógica pedagógica:\n",
    "- Revisar o ciclo de **forward → loss → backward → step**;\n",
    "- Mostrar como o **autograd** substitui a derivação manual;\n",
    "- Comparar os blocos do código manual com suas **contrapartes no PyTorch**;\n",
    "- Treinar uma MLP simples em dados sintéticos (default) **ou** no dataset BuonoPreço.\n",
    "\n",
    "> **Ideia-chave**: o que você fazia \"na mão\" agora é feito pelo framework — mas o **ciclo conceitual é o mesmo**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a497086",
   "metadata": {},
   "source": [
    "\n",
    "## Mapa mental: manual vs PyTorch\n",
    "\n",
    "| Conceito | Versão manual | Em PyTorch |\n",
    "|---|---|---|\n",
    "| Tensor/arranjos | `numpy.ndarray` | `torch.Tensor` (CPU/GPU/MPS) |\n",
    "| Forward | funções e `np.dot` | `nn.Module.forward` / `nn.Sequential` |\n",
    "| Loss | função escrita à mão (ex.: BCE) | `nn.BCEWithLogitsLoss` / `nn.CrossEntropyLoss` |\n",
    "| Gradientes | regras da cadeia manuais | `autograd` (`loss.backward()`) |\n",
    "| Atualização | `W -= lr * dW` | `optimizer.step()` (SGD/Adam) |\n",
    "| Mini-batch | laço manual | Mantive o laço, mas passaremos a usar o `DataLoader` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc55314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ambiente e Dispositivo ===\n",
    "import sys, os, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# CUDA (NVIDIA) -> MPS (Apple Silicon) -> CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d2679",
   "metadata": {},
   "source": [
    "\n",
    "## Definições\n",
    "- **Sintético (padrão)**: rápido para experimentar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configurações Gerais ===\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "HIDDEN = [64, 32]\n",
    "DROPOUT = 0.1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc087478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataset Sintético (binário) ===\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_moons(n_samples=3000, noise=0.25, random_state=SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    stratify=y, random_state=SEED)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "X_train_t.shape, y_train_t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ef5b5",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo (PyTorch)\n",
    "\n",
    "A MLP em PyTorch é uma classe que herda de `nn.Module`. No `__init__` definimos camadas; no `forward`, o fluxo de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa07ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP de 1 camada oculta (equivalente ao W1,b1,W2,b2)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out=1, seed=42):\n",
    "        super().__init__()\n",
    "        # Inicialização ~ 1/sqrt(fan_in), como no init_params\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        self.l1 = nn.Linear(n_in, n_hidden)\n",
    "        with torch.no_grad():\n",
    "            nn.init.normal_(self.l1.weight, mean=0.0, std=1.0/math.sqrt(n_in), generator=g)\n",
    "            nn.init.constant_(self.l1.bias, 0.0)\n",
    "\n",
    "        self.l2 = nn.Linear(n_hidden, n_out)\n",
    "        with torch.no_grad():\n",
    "            nn.init.normal_(self.l2.weight, mean=0.0, std=1.0/math.sqrt(n_hidden), generator=g)\n",
    "            nn.init.constant_(self.l2.bias, 0.0)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.l1(x)\n",
    "        a1 = self.act(z1)\n",
    "        z2 = self.l2(a1)  # logit (sem sigmoid aqui; usaremos BCEWithLogitsLoss)\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cddc7",
   "metadata": {},
   "source": [
    "## Laço de treinamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(X, y, n_hidden=8, lr=0.1, epochs=100, seed=42, verbose=False):\n",
    "    \"\"\"\n",
    "    Treino \"full-batch\" para ficar próximo do visão guiada base.\n",
    "    X: np.ndarray [N, D]\n",
    "    y: np.ndarray [N] ou [N,1] (binário: 0/1)\n",
    "    \"\"\"\n",
    "    # Tensores\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y = y.reshape(-1, 1) if y.ndim == 1 else y\n",
    "    y_t = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Modelo\n",
    "    model = MLP(n_in=X_t.shape[1], n_hidden=n_hidden, n_out=1, seed=seed).to(device)\n",
    "    \n",
    "    # Loss: BCE com logits (não aplique sigmoid no forward)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Otimizador: SGD para equivaler ao sgd_step\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X_t).squeeze(1)\n",
    "        loss   = criterion(logits, y_t.squeeze(1))\n",
    "\n",
    "        # autograd calcula gradientes (substitui backward)\n",
    "        loss.backward()\n",
    "        # aplica atualização (substitui sgd_step)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        if verbose and (ep % max(1, epochs//10) == 0):\n",
    "            print(f\"época {ep:4d} | loss={losses[-1]:.6f}\")\n",
    "\n",
    "    return model, np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f02b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar com diferentes taxas de aprendizado\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1]:\n",
    "    model, losses = train_torch(X, y, n_hidden=8, lr=0.1, epochs=100, verbose=True)\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"BCE\")\n",
    "    plt.title(f\"Curva de treino (lr={lr})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b521",
   "metadata": {},
   "source": [
    "\n",
    "## Exercícios sugeridos (para fixação)\n",
    "1. **Troque o otimizador** para `SGD` e compare as curvas.  \n",
    "2. **Ajuste a arquitetura** (`HIDDEN`, `DROPOUT`) e avalie impacto.  \n",
    "3. **Troque a loss** para `nn.BCELoss` (com `sigmoid` explícito) e comente as diferenças.  \n",
    "4. **Salve e carregue** pesos com `torch.save` / `load_state_dict`:\n",
    "\n",
    "```Python\n",
    "# === Salvamento do modelo e metadados ===\n",
    "save_dir = \"model/mlp_visao_guiada_pytorch\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(save_dir, \"mlp_visao_guiada_pytorch_state.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "prep_path = os.path.join(save_dir, \"prep_stats.npz\")\n",
    "np.savez(prep_path, mu=mu, std=std, in_dim=in_dim, n_classes=n_classes)\n",
    "\n",
    "print(\"Salvo:\", model_path)\n",
    "print(\"Salvo:\", prep_path)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80d220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
