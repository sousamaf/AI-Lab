{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfe88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# MLP simples em PyTorch (1 camada oculta)\n",
    "def make_mlp(n_in, n_hidden, n_out=1, dropout=0.0):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(n_in, n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(n_hidden, n_out)  # saída = logit (sem sigmoid aqui)\n",
    "    ).to(device)\n",
    "\n",
    "# Função de treino minimalista\n",
    "def train(X, y, n_hidden=8, lr=0.1, epochs=100, verbose=False):\n",
    "    # Tensores\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    y_t = torch.tensor(y.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "\n",
    "    model = make_mlp(X_t.shape[1], n_hidden, n_out=1)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()             # Loss binária\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # SGD puro\n",
    "\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_t)\n",
    "        loss = criterion(logits, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if verbose and (ep % max(1, epochs//10) == 0):\n",
    "            print(f\"época {ep:4d} | loss={loss.item():.6f}\")\n",
    "\n",
    "    return model, losses"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
