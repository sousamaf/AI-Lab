{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba35ca8",
   "metadata": {},
   "source": [
    "# Arquitetura e Treinamento de uma MLP (visão guiada)\n",
    "\n",
    "**Objetivo da aula (30–45 min):**\n",
    "- Entender a arquitetura básica de uma MLP para classificação binária.\n",
    "- Compreender o *forward*, *loss* (BCE), *backprop* e um laço de treino em **NumPy**.\n",
    "- Realizar micro-exercícios para fixar conceitos e leitura de *shapes*.\n",
    "\n",
    "**Plano rápido**\n",
    "1. Camada densa e não linearidade.\n",
    "2. Ativações (`ReLU`, `Sigmoid`) e gradientes.\n",
    "3. *Forward* vetorizado (2 camadas).\n",
    "4. Função de custo BCE + métrica de acurácia.\n",
    "5. *Backpropagation* vetorizada.\n",
    "6. *Training loop* curto e observação da curva de perda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e76488",
   "metadata": {},
   "source": [
    "## 1) Camada densa mínima (conceito)\n",
    "\n",
    "A camada densa (totalmente conectada) aplica \\(Z = XW + b\\).  \n",
    "Se **X** tem forma `(m, d)` e a camada possui `h` neurônios, então **W** tem forma `(d, h)` e **b** tem forma `(1, h)`.\n",
    "\n",
    "**Micro-exercício 1 (responda em texto em uma célula abaixo):**\n",
    "- Se a entrada tem `d=6` atributos e a camada oculta tem `h=10` neurônios, qual a forma de `W` e `b`?\n",
    "- Explique por que a presença de uma **não linearidade** entre camadas é necessária para modelar funções não lineares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7f558",
   "metadata": {},
   "source": [
    "## 2) Funções de ativação e derivadas\n",
    "\n",
    "Vamos implementar `ReLU` e `Sigmoid`, além de seus gradientes. Observe que o gradiente da Sigmoid pode ser expresso em função da **saída ativada**:\n",
    "\n",
    "Seja \n",
    "$$a = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "então a derivada é:\n",
    "$$\\frac{d\\,\\sigma(z)}{dz} = \\sigma(z)\\,(1-\\sigma(z)) = a(1-a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43674888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_grad(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_grad(a):\n",
    "    # a = sigmoid(z)\n",
    "    return a * (1.0 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a87ca",
   "metadata": {},
   "source": [
    "**Micro-exercício 2:**  \n",
    "Por que é prático implementar `sigmoid_grad` recebendo a **ativação `a`** em vez de `z`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3918e2b",
   "metadata": {},
   "source": [
    "## 3) Forward propagation (duas camadas)\n",
    "\n",
    "Consideraremos uma MLP com **1 camada oculta** (ReLU) e **camada de saída** (Sigmoid) para classificação binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e504bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    W1, b1 = params[\"W1\"], params[\"b1\"]\n",
    "    W2, b2 = params[\"W2\"], params[\"b2\"]\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = sigmoid(Z2)  # probabilidade classe=1\n",
    "    cache = {\"X\": X, \"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95d66b",
   "metadata": {},
   "source": [
    "**Micro-exercício 3:**  \n",
    "Onde acontece a **não linearidade** no `forward` e qual o seu papel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b4c6b",
   "metadata": {},
   "source": [
    "## 4) Função de custo (BCE) e métrica de acurácia\n",
    "\n",
    "Usaremos a **Binary Cross-Entropy** (BCE) estável numericamente. Para evitar `log(0)`, aplicamos *clipping* nas probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_true, y_pred, eps=1e-12):\n",
    "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    return -np.mean(y_true*np.log(y_pred) + (1.0 - y_true)*np.log(1.0 - y_pred))\n",
    "\n",
    "def accuracy(y_true, y_pred, thr=0.5):\n",
    "    return np.mean((y_pred >= thr) == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21527447",
   "metadata": {},
   "source": [
    "**Micro-exercício 4:**  \n",
    "Explique o papel do `np.clip` na `bce_loss`. O que pode acontecer sem esse cuidado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f885b8c",
   "metadata": {},
   "source": [
    "## 5) Backpropagation (derivadas em cadeia)\n",
    "\n",
    "Abaixo implementamos os gradientes vetorizados para uma MLP de duas camadas (BCE + Sigmoid na saída).  \n",
    "Observação: dividimos por `m` (tamanho do lote) para obter o gradiente **médio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y_true, cache, params):\n",
    "    W2 = params[\"W2\"]\n",
    "    X, Z1, A1, Z2, A2 = cache[\"X\"], cache[\"Z1\"], cache[\"A1\"], cache[\"Z2\"], cache[\"A2\"]\n",
    "    m = y_true.shape[0]\n",
    "\n",
    "    # dL/dZ2 para BCE+Sigmoid com y_pred = A2\n",
    "    dZ2 = (A2 - y_true) / m\n",
    "    dW2 = A1.T @ dZ2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = dZ2 @ W2.T\n",
    "    dZ1 = dA1 * relu_grad(Z1)\n",
    "    dW1 = X.T @ dZ1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b6a8a",
   "metadata": {},
   "source": [
    "**Micro-exercício 5:**  \n",
    "Por que usamos a **regra da cadeia** para obter `dZ1` a partir de `dA1`? Explique o papel de `relu_grad(Z1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f1cfc",
   "metadata": {},
   "source": [
    "## 6) Laço de treinamento curto + curvas\n",
    "\n",
    "Vamos inicializar os parâmetros, treinar por poucas épocas e observar a curva de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa145cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_in, n_hidden, n_out, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # inicialização com variância ~ 1/fan_in\n",
    "    W1 = rng.normal(0, 1.0/np.sqrt(n_in), size=(n_in, n_hidden))\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    W2 = rng.normal(0, 1.0/np.sqrt(n_hidden), size=(n_hidden, n_out))\n",
    "    b2 = np.zeros((1, n_out))\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "def sgd_step(params, grads, lr=0.1):\n",
    "    for key in [\"W1\",\"b1\",\"W2\",\"b2\"]:\n",
    "        params[key] -= lr * grads[\"d\"+key]\n",
    "\n",
    "def train(X, y, n_hidden=8, lr=0.1, epochs=100, seed=42, verbose=False):\n",
    "    params = init_params(X.shape[1], n_hidden, 1, seed=seed)\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        y_pred, cache = forward(X, params)\n",
    "        loss = bce_loss(y, y_pred)\n",
    "        grads = backward(y, cache, params)\n",
    "        sgd_step(params, grads, lr)\n",
    "        losses.append(loss)\n",
    "        if verbose and (ep % max(1, epochs//10) == 0):\n",
    "            print(f\"época {ep:4d} | loss={loss:.6f}\")\n",
    "    return params, np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4e698",
   "metadata": {},
   "source": [
    "**Dados sintéticos para demonstração**  \n",
    "(Aqui criamos um conjunto simples apenas para visualizar o comportamento do treinamento.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f212be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados sintéticos binários (duas nuvens separáveis com ruído leve)\n",
    "rng = np.random.default_rng(0)\n",
    "m = 400\n",
    "X_pos = rng.normal([2.0, 2.0], [1.0, 1.0], size=(m//2, 2))\n",
    "X_neg = rng.normal([-2.0, -2.0], [1.0, 1.0], size=(m//2, 2))\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.vstack([np.ones((m//2,1)), np.zeros((m//2,1))])\n",
    "\n",
    "# Embaralhar\n",
    "idx = rng.permutation(m)\n",
    "X, y = X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ef9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar com diferentes taxas de aprendizado\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1]:\n",
    "    _, losses = train(X, y, n_hidden=8, lr=lr, epochs=200, seed=0, verbose=False)\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"BCE\")\n",
    "    plt.title(f\"Curva de treino (lr={lr})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8bf3ef",
   "metadata": {},
   "source": [
    "**Micro-exercício 6:**  \n",
    "Compare o comportamento da curva de perda para `lr = 0.01`, `0.05` e `0.1`.  \n",
    "- Qual apresenta convergência mais lenta?  \n",
    "- Em algum caso a perda oscila? O que isso sugere sobre a escolha de `lr`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43dee1",
   "metadata": {},
   "source": [
    "## 7) Checklist mental (projeto de RNAs)\n",
    "\n",
    "- **Entradas/saídas**: tipos, escalas e codificação dos rótulos.\n",
    "- **Arquitetura**: número de camadas e neurônios, ativação.\n",
    "- **Objetivo**: função de perda e métricas.\n",
    "- **Treinamento**: inicialização, `lr`, épocas, regularização.\n",
    "- **Avaliação**: acurácia, matriz de confusão, inspeção de erros.\n",
    "- **Iteração**: experimente `n_hidden`, `lr` e inicializações diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdab1aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
