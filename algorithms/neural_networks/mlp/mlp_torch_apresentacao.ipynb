{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2885de8",
   "metadata": {},
   "source": [
    "# MLP com PyTorch (roteiro + código)\n",
    "Introdução ao PyTorch para construção de redes: compreender a abstração tensor + autograd e treinar uma MLP simples, salvando e carregando o modelo.\n",
    "\n",
    "> Este notebook foi escrito para ser **didático** e **autocontido**. Rode célula a célula durante a aula ou use como material de estudo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação rápida\n",
    "try:\n",
    "    import torch, torch.nn as nn, torch.optim as optim\n",
    "    TORCH_OK = True\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "except Exception as e:\n",
    "    TORCH_OK = False\n",
    "    print(\"PyTorch não está disponível neste ambiente.\\n\"\n",
    "          \"O notebook permanece útil para estudo; execute localmente onde o PyTorch esteja instalado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954311a9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Conceito central do PyTorch: **tensor + autograd**\n",
    "\n",
    "- **Tensor** é a estrutura de dados principal (como `ndarray` do NumPy), com suporte a GPU.\n",
    "- **Autograd** registra operações e calcula **gradientes automaticamente** via `.backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if TORCH_OK:\n",
    "    import torch\n",
    "\n",
    "# Comparação rápida NumPy x Torch\n",
    "a = np.array([1.0, 2.0, 3.0])\n",
    "print(\"NumPy * 2:\", a * 2)\n",
    "\n",
    "if TORCH_OK:\n",
    "    b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    print(\"Torch * 2:\", b * 2)\n",
    "\n",
    "    # Exemplo de autograd\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x ** 2           # y = x^2\n",
    "    y.backward()         # dy/dx = 2x -> 4\n",
    "    print(\"x.grad =\", x.grad.item())\n",
    "else:\n",
    "    print(\"Demonstração do autograd não executada (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d0fcc",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Construindo modelos com `nn.Module`\n",
    "\n",
    "A ideia é compor blocos reutilizáveis: camadas (`nn.Linear`), ativações (`nn.ReLU`, `nn.Sigmoid`) e um `forward` que descreve o fluxo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_OK:\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class Perceptron(nn.Module):\n",
    "        def __init__(self, in_features):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(in_features, 1)\n",
    "            self.act = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            return self.act(self.fc(x))\n",
    "\n",
    "    class MLP_Iris(nn.Module):\n",
    "        def __init__(self, in_features=4, hidden=8, out_features=3):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_features, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, out_features)  # logits\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)  # CrossEntropyLoss espera logits (sem Softmax)\n",
    "\n",
    "    print(\"Exemplos de modelos prontos:\")\n",
    "    print(Perceptron(4))\n",
    "    print(MLP_Iris())\n",
    "else:\n",
    "    print(\"Definições de modelo não executadas (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df1bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Dataset simples: **Iris** (scikit-learn)\n",
    "\n",
    "Usaremos o Iris para classificar 3 espécies. É um dataset pequeno e clássico — perfeito para uma primeira MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int64)  # 0,1,2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87514775",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_OK:\n",
    "    import torch\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train)\n",
    "    y_train_t = torch.from_numpy(y_train)\n",
    "    X_test_t  = torch.from_numpy(X_test)\n",
    "    y_test_t  = torch.from_numpy(y_test)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=32)\n",
    "    print(\"Loaders prontos.\")\n",
    "else:\n",
    "    print(\"Dataloaders não criados (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388b025",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Loop de treinamento (SGD + CrossEntropyLoss)\n",
    "\n",
    "Fluxo: **forward → loss → backward → step**.  \n",
    "A CrossEntropyLoss já inclui o Softmax internamente (por isso usamos logits na saída do modelo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time\n",
    "\n",
    "if TORCH_OK:\n",
    "    model = MLP_Iris(in_features=4, hidden=8, out_features=3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "    def accuracy(loader):\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                logits = model(xb)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    EPOCHS = 120\n",
    "    history = {\"loss\": [], \"acc_train\": [], \"acc_test\": []}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        tr_acc = accuracy(train_loader)\n",
    "        te_acc = accuracy(test_loader)\n",
    "        history[\"loss\"].append(epoch_loss)\n",
    "        history[\"acc_train\"].append(tr_acc)\n",
    "        history[\"acc_test\"].append(te_acc)\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            print(f\"Época {epoch:3d}/{EPOCHS} | loss={epoch_loss:.4f} | acc_tr={tr_acc:.3f} | acc_te={te_acc:.3f}\")\n",
    "    t1 = time.time()\n",
    "    print(f\"Treino concluído em {t1 - t0:.2f}s\")\n",
    "else:\n",
    "    print(\"Treinamento não executado (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e57408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if TORCH_OK and len(history[\"loss\"]) > 0:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history[\"loss\"], label=\"Loss (train)\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de perda\"); plt.grid(True); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history[\"acc_train\"], label=\"Acurácia treino\")\n",
    "    plt.plot(history[\"acc_test\"], label=\"Acurácia teste\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Acurácia\"); plt.title(\"Evolução da acurácia\"); plt.grid(True); plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Sem histórico para plotes (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1e61e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Salvar e carregar o modelo\n",
    "\n",
    "Use `state_dict` para salvar apenas os **pesos** (boa prática para compartilhar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if TORCH_OK:\n",
    "    save_path = \"mlp_iris_state_dict.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"Modelo salvo em:\", save_path)\n",
    "\n",
    "    # Carregar em um novo objeto com a mesma arquitetura\n",
    "    reloaded = MLP_Iris(in_features=4, hidden=8, out_features=3)\n",
    "    reloaded.load_state_dict(torch.load(save_path, map_location=\"cpu\"))\n",
    "    reloaded.eval()\n",
    "    print(\"Modelo recarregado. Exemplo de print(model):\")\n",
    "    print(reloaded)\n",
    "else:\n",
    "    print(\"Salvar/carregar não executado (PyTorch indisponível).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb481a16",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Visualização da rede\n",
    "\n",
    "- `print(model)` exibe a **estrutura das camadas**.\n",
    "- Para visualizar o **grafo computacional**, é possível usar a biblioteca externa `torchviz`.\n",
    "\n",
    "> Exemplo (opcional, requer `torchviz` instalado):\n",
    "```python\n",
    "from torchviz import make_dot\n",
    "x = torch.randn(1, 4)\n",
    "make_dot(model(x), params=dict(model.named_parameters()))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a4f72",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Conclusão orientada ao negócio\n",
    "\n",
    "Mesmo com uma MLP pequena, observamos a redução do erro e bom desempenho no conjunto de teste. Na prática, modelos assim podem apoiar tarefas como **classificação de perfis** e **priorização de ações**. Ao salvar e recarregar o modelo, abrimos caminho para **implantação** e **reprodutibilidade**, permitindo que times usem o modelo em outros sistemas e validem resultados de forma consistente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
